# Douga Video Editor - Complete AI Integration Specification

> Comprehensive reference for AI assistants working with the Douga video editing API.
> For quick reference, see llms.txt

## Table of Contents

### Core Reference
1. [Architecture Overview](#architecture-overview)
2. [Terminology Definitions](#terminology-definitions)
3. [Schema Map](#schema-map-quick-reference)
4. [Information Hierarchy](#information-hierarchy)
5. [API Reference](#api-reference)
6. [Data Models](#data-models)

### Operations
7. [Semantic Operations](#semantic-operations)
8. [Batch Operations](#batch-operations)
9. [Analysis Tools](#analysis-tools)
10. [Preview & Inspection](#preview--inspection)

### Error Handling
11. [Parameter Reference](#parameter-reference-complete)
12. [Error Code Reference](#error-code-reference)
13. [Failure Modes & Recovery](#failure-modes--recovery-catalog)

### AI Behavior
14. [Ambiguity Resolution & Confidence](#ambiguity-resolution--confidence)
15. [Multi-turn Conversation Context](#multi-turn-conversation-context)
16. [Edge Cases](#edge-cases)
17. [Partial Failure Handling](#partial-failure-handling)
18. [Concurrent Editing](#concurrent-editing)

### Advanced
19. [Large Projects](#large-projects)
20. [Frame Precision](#frame-precision)
21. [Visual Verification](#visual-verification)
22. [Udemy-Specific Requirements](#udemy-specific-requirements)

### Planning
23. [Validation Mode (Future)](#validation-mode-future)
24. [Rollback & History (Future)](#rollback--history-future)
25. [Best Practices](#best-practices)
26. [Example Workflows](#example-workflows)

---

## Architecture Overview

### System Components

```
┌─────────────────────────────────────────────────────────────┐
│                    AI Assistant (Claude)                    │
└─────────────────────────┬───────────────────────────────────┘
                          │ HTTP/JSON
                          ▼
┌─────────────────────────────────────────────────────────────┐
│                    Douga Backend API                        │
│                    /api/ai/* endpoints                      │
└─────────────────────────┬───────────────────────────────────┘
                          │
          ┌───────────────┼───────────────┐
          ▼               ▼               ▼
   ┌──────────┐    ┌──────────┐    ┌──────────┐
   │ Projects │    │  Assets  │    │  Render  │
   │   (DB)   │    │  (GCS)   │    │ Pipeline │
   └──────────┘    └──────────┘    └──────────┘
```

### Timeline Structure

```
┌─────────────────────────────────────────────────────────────┐
│  Video Layers (5 layers, bottom to top)                     │
├─────────────────────────────────────────────────────────────┤
│ L5: text       │ テロップ・字幕                              │
│ L4: effects    │ エフェクト（キラキラ、集中線）              │
│ L3: avatar     │ アバター（クロマキー合成）                  │
│ L2: content    │ 操作画面・スライド                          │
│ L1: background │ 背景（3D空間、グラデーション）              │
├─────────────────────────────────────────────────────────────┤
│  Audio Tracks (3 tracks)                                    │
├─────────────────────────────────────────────────────────────┤
│ narration      │ ナレーション音声                            │
│ bgm            │ BGM（ダッキング対応）                       │
│ se             │ 効果音                                      │
└─────────────────────────────────────────────────────────────┘
```

---

## Terminology Definitions

> Precise definitions for video editing terms used in this API.

### Timeline Terms

| Term | Japanese | Definition |
|------|----------|------------|
| **Clip** | クリップ | A segment of video/audio placed on the timeline |
| **Layer** | レイヤー | A horizontal track for video clips (stacked vertically) |
| **Track** | トラック | A horizontal track for audio clips |
| **Timeline** | タイムライン | The complete project arrangement of all clips |
| **Playhead** | 再生ヘッド | The current position indicator on the timeline |
| **Duration** | デュレーション | Length of a clip or project in milliseconds |

### Timing Terms

| Term | Japanese | Definition |
|------|----------|------------|
| **start_ms** | 開始位置 | When a clip begins on the timeline (ms from 0) |
| **end_ms** | 終了位置 | When a clip ends on the timeline (start_ms + duration_ms) |
| **duration_ms** | 長さ | How long the clip plays (end_ms - start_ms) |
| **in_point_ms** | イン点 | Where to start playing from within the source asset |
| **out_point_ms** | アウト点 | Where to stop playing within the source asset |
| **Trim** | トリム | Using only a portion of source asset (via in/out points) |

### Transform Terms

| Term | Japanese | Definition |
|------|----------|------------|
| **x, y** | 座標 | Position from canvas center (0,0 = center) |
| **scale** | スケール | Size multiplier (1.0 = original, 2.0 = double size) |
| **rotation** | 回転 | Clockwise rotation in degrees (-360 to 360) |
| **anchor** | アンカー | Reference point for transforms (center, top-left, etc.) |
| **Crop** | クロップ | Cutting edges of the frame (different from trim) |

### Effect Terms

| Term | Japanese | Definition |
|------|----------|------------|
| **opacity** | 不透明度 | Transparency level (0.0 = invisible, 1.0 = opaque) |
| **Chroma key** | クロマキー | Making a color transparent (typically green screen) |
| **Blend mode** | ブレンドモード | How clip combines with layers below (normal, multiply, etc.) |
| **Ducking** | ダッキング | Auto-lowering BGM volume when narration plays |
| **Fade** | フェード | Gradual opacity transition (fade in/out) |

### Asset Terms

| Term | Japanese | Definition |
|------|----------|------------|
| **Asset** | アセット | Source media file (video, audio, image) |
| **Source duration** | 素材の長さ | Original length of the asset file |
| **Resolution** | 解像度 | Pixel dimensions (e.g., 1920x1080) |
| **FPS** | フレームレート | Frames per second (Douga uses 30fps) |

### Operation Terms

| Term | Japanese | Definition |
|------|----------|------------|
| **Snap** | スナップ | Auto-align to clip edges or time markers |
| **Ripple** | リップル | Moving subsequent clips when one is resized/moved |
| **Gap** | ギャップ | Empty space between clips on same layer |
| **Group** | グループ | Linked clips that move together |
| **Semantic op** | セマンティック操作 | High-level operation (close_gap vs manual moves) |

### Important Distinctions

| Confused Terms | Difference |
|----------------|------------|
| Trim vs Crop | Trim = cut time (in/out points), Crop = cut space (edges) |
| Layer vs Track | Layer = video (5 stacked), Track = audio (3 parallel) |
| Duration vs out_point | Duration = clip length, out_point = position in source |
| Scale vs Resolution | Scale = display size, Resolution = pixel count |

---

## Information Hierarchy

### Why Hierarchy Matters

AI assistants can hallucinate IDs, timestamps, or properties they haven't seen.
The information hierarchy ensures you always have verified data before acting.

### Level Definitions

| Level | Name | Token Budget | Purpose |
|-------|------|--------------|---------|
| L1 | Summary | ~300 tokens | Project scope, rough counts |
| L2 | Structure | ~800 tokens | Layer/track IDs, time coverage |
| L3 | Details | ~400/clip | Full clip properties, neighbors |
| L4 | Raw | Unlimited | Full timeline JSON (avoid) |

### Recommended Access Pattern

```
1. GET /overview  →  Understand scale of project
   ↓
2. GET /structure →  Find layer/track IDs, see where content is
   ↓
3. GET /clip/{id} →  Get specific clip details when needed
   ↓
4. POST/PATCH     →  Make changes with verified IDs
```

### What Each Level Returns

**L1 Overview Response:**
```json
{
  "project": {
    "name": "Udemy Lecture 1",
    "duration_ms": 180000,
    "dimensions": "1920x1080",
    "fps": 30,
    "status": "draft"
  },
  "summary": {
    "layer_count": 5,
    "audio_track_count": 3,
    "total_video_clips": 12,
    "total_audio_clips": 8,
    "total_assets_used": 6
  },
  "last_modified": "2025-01-15T10:30:00Z"
}
```

**L2 Structure Response:**
```json
{
  "project_id": "uuid",
  "duration_ms": 180000,
  "layers": [
    {
      "id": "layer-uuid-1",
      "name": "Background",
      "type": "background",
      "clip_count": 1,
      "time_coverage": [{"start_ms": 0, "end_ms": 180000}],
      "visible": true,
      "locked": false
    },
    {
      "id": "layer-uuid-2",
      "name": "Avatar",
      "type": "avatar",
      "clip_count": 3,
      "time_coverage": [
        {"start_ms": 0, "end_ms": 60000},
        {"start_ms": 90000, "end_ms": 180000}
      ],
      "visible": true,
      "locked": false
    }
  ],
  "audio_tracks": [
    {
      "id": "track-uuid-1",
      "name": "Narration",
      "type": "narration",
      "clip_count": 5,
      "time_coverage": [...],
      "volume": 1.0,
      "muted": false,
      "ducking_enabled": false
    }
  ]
}
```

**L3 Clip Details Response:**
```json
{
  "id": "clip-uuid",
  "layer_id": "layer-uuid-2",
  "layer_name": "Avatar",
  "asset_id": "asset-uuid",
  "asset_name": "avatar_red.mp4",
  "timing": {
    "start_ms": 0,
    "duration_ms": 60000,
    "end_ms": 60000,
    "in_point_ms": 0,
    "out_point_ms": 60000
  },
  "transform": {
    "x": 400,
    "y": -200,
    "width": null,
    "height": null,
    "scale": 0.5,
    "rotation": 0,
    "anchor": "center"
  },
  "effects": {
    "opacity": 1.0,
    "blend_mode": "normal",
    "chroma_key_enabled": true,
    "chroma_key_color": "#00FF00"
  },
  "transition_in": {"type": "fade", "duration_ms": 500},
  "transition_out": {"type": "none", "duration_ms": 0},
  "text_content": null,
  "group_id": null,
  "previous_clip": null,
  "next_clip": {
    "id": "clip-uuid-2",
    "start_ms": 90000,
    "end_ms": 150000,
    "gap_ms": 30000
  }
}
```

---

## Schema Map (Quick Reference)

> Token-efficient navigation guide for AI assistants.

### Entity Relationship Diagram

```
                            ┌─────────────────┐
                            │    PROJECT      │
                            │  (id, name,     │
                            │   duration_ms)  │
                            └────────┬────────┘
                                     │
              ┌──────────────────────┼──────────────────────┐
              │                      │                      │
              ▼                      ▼                      ▼
     ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐
     │     LAYER       │   │   AUDIO_TRACK   │   │     ASSET       │
     │  (id, type,     │   │  (id, type,     │   │  (id, type,     │
     │   name, order)  │   │   name, volume) │   │   duration_ms)  │
     └────────┬────────┘   └────────┬────────┘   └─────────────────┘
              │                      │                      ▲
              │ 1:N                  │ 1:N                  │ ref
              ▼                      ▼                      │
     ┌─────────────────┐   ┌─────────────────┐            │
     │   VIDEO_CLIP    │   │   AUDIO_CLIP    │────────────┘
     │  (id, timing,   │   │  (id, timing,   │
     │   transform,    │   │   volume, fade) │
     │   effects)      │   └─────────────────┘
     └────────┬────────┘
              │
              │ 1:N (optional)
              ▼
     ┌─────────────────┐
     │    KEYFRAME     │
     │  (time_ms,      │
     │   property,     │
     │   value)        │
     └─────────────────┘
```

### Entity Quick Reference

| Entity | Key Fields | Get Via | Token Cost |
|--------|------------|---------|------------|
| Project | id, duration_ms, fps | L1 overview | ~300 |
| Layer | id, type, name, clip_count | L2 structure | ~100/layer |
| AudioTrack | id, type, name, volume | L2 structure | ~80/track |
| VideoClip | id, timing, transform, effects | L3 clip/{id} | ~400 |
| AudioClip | id, timing, volume, fade | L3 audio-clip/{id} | ~300 |
| Asset | id, type, filename, duration_ms | L2 assets | ~50/asset |

### Navigation Paths

```
To add a clip:
  L2 /structure → get layer_id
  L2 /assets → get asset_id
  POST /clips (layer_id, asset_id, start_ms, duration_ms)

To modify a clip:
  L2 /structure → find clip in layer.clips
  L3 /clip/{id} → get current state
  PATCH /clip/{id}/transform or /effects

To find what's playing:
  L2 /at-time/{ms} → get active clips at that moment

To check gaps:
  L2 /analysis/gaps → find empty time ranges
```

### ID Formats

| Entity | Format | Example |
|--------|--------|---------|
| Project | UUID | `a1b2c3d4-e5f6-...` |
| Layer | UUID | `8f2c1d3e-4a5b-...` |
| VideoClip | UUID | `2c9a7f1b-9d0e-...` |
| AudioTrack | UUID | `5b6c7d8e-0f1a-...` |
| AudioClip | UUID | `7e8f9a0b-1c2d-...` |
| Asset | UUID | `9a0b1c2d-3e4f-...` |

---

## API Reference

### Base URL

```
/api/ai/
```

### Authentication

All endpoints require a valid Firebase token:
```
Authorization: Bearer <firebase-token>
```

In development mode (`DEV_MODE=true`), use:
```
Authorization: Bearer dev-token
```

### Schema Discovery

```
GET /api/ai/schemas

Returns list of available schemas with token estimates.
AI assistants should call this first to understand API capabilities.
```

### L1 Endpoints

#### Get Project Overview
```
GET /api/ai/project/{project_id}/overview

Response: L1ProjectOverview
Token budget: ~300 tokens

Use when: Starting work on a project, understanding scope
```

### L2 Endpoints

#### Get Timeline Structure
```
GET /api/ai/project/{project_id}/structure

Response: L2TimelineStructure
Token budget: ~800 tokens

Use when: Finding layer/track IDs, understanding layout
```

#### Get Timeline at Time
```
GET /api/ai/project/{project_id}/at-time/{time_ms}

Response: L2TimelineAtTime
Token budget: ~400 tokens

Use when: Understanding what's playing at a specific moment
```

#### Get Asset Catalog
```
GET /api/ai/project/{project_id}/assets

Response: L2AssetCatalog
Token budget: ~50 tokens per asset

Use when: Finding asset IDs to use in new clips
```

### L3 Endpoints

#### Get Video Clip Details
```
GET /api/ai/project/{project_id}/clip/{clip_id}

Response: L3ClipDetails
Token budget: ~400 tokens

Use when: Need full clip properties before modification
```

#### Get Audio Clip Details
```
GET /api/ai/project/{project_id}/audio-clip/{clip_id}

Response: L3AudioClipDetails
Token budget: ~400 tokens

Use when: Need audio clip properties before modification
```

### Write Endpoints: Layers

#### Add Layer
```
POST /api/ai/project/{project_id}/layers

Request Body:
{
  "name": "Layer Name",          // Required
  "type": "content",             // Optional: background, content, avatar, effects, text
  "insert_at": null              // Optional: position (0=top, null=bottom)
}

Response: LayerSummary (201 Created)
```

#### Update Layer (Rename / Visibility / Lock)
```
PATCH /api/ai/project/{project_id}/layer/{layer_id}

Request Body:
{
  "name": "New Name",            // Optional: new layer name
  "visible": true,               // Optional: layer visibility
  "locked": false                // Optional: lock status
}

Response: LayerSummary
```

#### Reorder Layers
```
PUT /api/ai/project/{project_id}/layers/order

Request Body:
{
  "layer_ids": ["layer-uuid-1", "layer-uuid-2", ...]  // New order (top to bottom)
}

Response: list[LayerSummary]
```

### Write Endpoints: Video Clips

#### Add Video Clip
```
POST /api/ai/project/{project_id}/clips

Request Body:
{
  "layer_id": "uuid",           // Required: from L2 structure
  "asset_id": "uuid",           // Optional: null for text clips
  "start_ms": 0,                // Required: >= 0
  "duration_ms": 3000,          // Required: > 0
  "in_point_ms": 0,             // Optional: trim start
  "out_point_ms": null,         // Optional: trim end
  "x": 0,                       // Optional: transform x
  "y": 0,                       // Optional: transform y
  "scale": 1.0,                 // Optional: transform scale
  "text_content": null,         // Optional: for text clips
  "text_style": null,           // Optional: text styling
  "group_id": null              // Optional: link clips
}

Response: L3ClipDetails (201 Created)
```

#### Add Audio Clip
```
POST /api/ai/project/{project_id}/audio-clips

Request Body:
{
  "track_id": "uuid",           // Required: from L2 structure
  "asset_id": "uuid",           // Required: audio asset
  "start_ms": 0,                // Required: >= 0
  "duration_ms": 3000,          // Required: > 0
  "in_point_ms": 0,             // Optional
  "out_point_ms": null,         // Optional
  "volume": 1.0,                // Optional: 0.0-2.0
  "fade_in_ms": 0,              // Optional
  "fade_out_ms": 0,             // Optional
  "group_id": null              // Optional
}

Response: L3AudioClipDetails (201 Created)
```

#### Move Video Clip
```
PATCH /api/ai/project/{project_id}/clip/{clip_id}/move

Request Body:
{
  "new_start_ms": 5000,         // Required: new position
  "new_layer_id": null          // Optional: move to different layer
}

Response: L3ClipDetails
```

#### Move Audio Clip
```
PATCH /api/ai/project/{project_id}/audio-clip/{clip_id}/move

Request Body:
{
  "new_start_ms": 5000,
  "new_track_id": null
}

Response: L3AudioClipDetails
```

#### Update Clip Transform
```
PATCH /api/ai/project/{project_id}/clip/{clip_id}/transform

Request Body:
{
  "x": 100,                     // Optional
  "y": -50,                     // Optional
  "width": 960,                 // Optional
  "height": 540,                // Optional
  "scale": 1.5,                 // Optional
  "rotation": 45,               // Optional: degrees
  "anchor": "center"            // Optional
}

Response: L3ClipDetails
```

#### Update Clip Effects
```
PATCH /api/ai/project/{project_id}/clip/{clip_id}/effects

Request Body:
{
  "opacity": 0.8,               // Optional: 0.0-1.0
  "blend_mode": "multiply",     // Optional
  "chroma_key_enabled": true,   // Optional
  "chroma_key_color": "#00FF00",// Optional
  "chroma_key_similarity": 0.4, // Optional: 0.0-1.0
  "chroma_key_blend": 0.1       // Optional: 0.0-1.0
}

Response: L3ClipDetails
```

#### Delete Video Clip
```
DELETE /api/ai/project/{project_id}/clip/{clip_id}

Response: 204 No Content
```

#### Delete Audio Clip
```
DELETE /api/ai/project/{project_id}/audio-clip/{clip_id}

Response: 204 No Content
```

---

## Data Models

### Transform

| Property | Type | Default | Range | Description |
|----------|------|---------|-------|-------------|
| x | float | 0 | -960 to 960 | Pixels from center |
| y | float | 0 | -540 to 540 | Pixels from center |
| width | float | null | 1-3840 | Override width |
| height | float | null | 1-2160 | Override height |
| scale | float | 1.0 | 0.1-10.0 | Uniform scale |
| rotation | float | 0 | 0-360 | Degrees |
| anchor | string | "center" | see below | Anchor point |

**Anchor values:** center, top-left, top-right, bottom-left, bottom-right

### Effects

| Property | Type | Default | Description |
|----------|------|---------|-------------|
| opacity | float | 1.0 | 0.0 (transparent) to 1.0 (opaque) |
| blend_mode | string | "normal" | normal, multiply, screen, overlay |
| chroma_key.enabled | bool | false | Enable green screen removal |
| chroma_key.color | string | "#00FF00" | Key color (hex) |
| chroma_key.similarity | float | 0.4 | Color match threshold |
| chroma_key.blend | float | 0.1 | Edge blending |

### Timing

| Property | Type | Description |
|----------|------|-------------|
| start_ms | int | Timeline position (milliseconds) |
| duration_ms | int | Clip length on timeline |
| in_point_ms | int | Start trim in source asset |
| out_point_ms | int | End trim in source asset |

**Note:** duration_ms on timeline can differ from asset duration when trimmed.

### Layer Types

| Type | Order | Purpose |
|------|-------|---------|
| background | 0 | Base layer (gradients, 3D backgrounds) |
| content | 1 | Main content (slides, screen captures) |
| avatar | 2 | Presenter avatar (usually with chroma key) |
| effects | 3 | Visual effects (particles, highlights) |
| text | 4 | Captions, titles, labels |

### Audio Track Types

| Type | Purpose | Default Volume |
|------|---------|----------------|
| narration | Voice-over | 1.0 |
| bgm | Background music | 0.3 |
| se | Sound effects | 0.8 |

---

## Semantic Operations

Semantic operations are high-level commands that handle complex logic internally.
They are safer and more reliable than raw edits.

### Available Operations

#### snap_to_previous
Move a clip to immediately follow the previous clip (close the gap).

```json
POST /api/ai/project/{id}/semantic
{
  "operation": "snap_to_previous",
  "target_clip_id": "clip-uuid"
}
```

#### snap_to_next
Move the next clip to immediately follow this clip.

```json
{
  "operation": "snap_to_next",
  "target_clip_id": "clip-uuid"
}
```

#### close_gap
Remove all gaps in a layer by shifting clips forward.

```json
{
  "operation": "close_gap",
  "target_layer_id": "layer-uuid"
}
```

#### auto_duck_bgm
Enable automatic BGM volume reduction when narration plays.

```json
{
  "operation": "auto_duck_bgm",
  "parameters": {
    "duck_to": 0.1,      // Volume during narration (0.0-1.0)
    "attack_ms": 200,    // Fade down duration
    "release_ms": 500    // Fade up duration
  }
}
```

**Note:** `duck_to` is a linear gain. Approximate dB change:  
`gain_db ≈ 20 * log10(duck_to)`  
LUFS alignment requires actual measurement; use this as a rough guide only.

#### rename_layer
Rename a layer (change its display name).

```json
POST /api/ai/project/{id}/semantic
{
  "operation": "rename_layer",
  "target_layer_id": "layer-uuid",
  "parameters": {
    "name": "New Layer Name"
  }
}
```

### Response Format

```json
{
  "success": true,
  "operation": "snap_to_previous",
  "changes_made": [
    "Moved clip from 5000ms to 3000ms (snapped to previous)"
  ],
  "affected_clip_ids": ["clip-uuid"],
  "error_message": null
}
```

---

## Analysis Tools

### Gap Analysis
```
GET /api/ai/project/{id}/analysis/gaps

Response:
{
  "total_gaps": 3,
  "total_gap_duration_ms": 15000,
  "gaps": [
    {
      "layer_or_track_id": "layer-uuid",
      "layer_or_track_name": "Content",
      "type": "video",
      "start_ms": 30000,
      "end_ms": 35000,
      "duration_ms": 5000
    }
  ]
}
```

### Pacing Analysis
```
GET /api/ai/project/{id}/analysis/pacing?segment_duration_ms=30000

Response:
{
  "overall_avg_clip_duration_ms": 5000,
  "segments": [
    {
      "start_ms": 0,
      "end_ms": 30000,
      "clip_count": 6,
      "avg_clip_duration_ms": 5000,
      "density": 0.2
    }
  ],
  "suggested_improvements": [
    "Segment 60s-90s has low clip density",
    "Segment 120s-150s has long clips - consider splitting"
  ]
}
```

---

## Batch Operations

Execute multiple operations in a single request for efficiency.

Supported operations: `add`, `move`, `update_transform`, `update_effects`, `delete`, `update_layer`

**Batch size limit:** use `/capabilities.max_batch_ops` as the hard limit.  
If not available, use **20** as a safe default and split requests.

```
POST /api/ai/project/{id}/batch

Request:
{
  "operations": [
    {
      "operation": "add",
      "clip_type": "video",
      "data": {
        "layer_id": "...",
        "asset_id": "...",
        "start_ms": 0,
        "duration_ms": 3000
      }
    },
    {
      "operation": "move",
      "clip_id": "existing-clip-id",
      "clip_type": "video",
      "data": {
        "new_start_ms": 5000
      }
    },
    {
      "operation": "update_layer",
      "layer_id": "layer-uuid",
      "data": {
        "name": "New Layer Name"
      }
    },
    {
      "operation": "delete",
      "clip_id": "clip-to-remove",
      "clip_type": "video"
    }
  ]
}

Response:
{
  "success": true,
  "total_operations": 4,
  "successful_operations": 4,
  "failed_operations": 0,
  "results": [
    {"operation": "add", "clip_id": "new-clip-uuid"},
    {"operation": "move", "clip_id": "existing-clip-id"},
    {"operation": "update_layer", "layer_id": "layer-uuid"},
    {"operation": "delete", "clip_id": "clip-to-remove"}
  ],
  "errors": []
}
```

---

## Best Practices

### 1. Always Verify Before Acting
```
❌ Bad: Assume clip ID from previous context
✅ Good: GET /structure, find ID, then act
```

### 2. Use Semantic Operations When Possible
```
❌ Bad: Calculate new start_ms manually, PATCH move
✅ Good: POST semantic with snap_to_previous
```

### 3. Check for Overlaps
```
The API will reject moves that cause overlaps.
Use L3 details to see neighbor gaps before planning moves.
```

### 4. Batch Related Operations
```
❌ Bad: 5 separate API calls for 5 clips
✅ Good: 1 batch request with 5 operations
```

### 5. Handle Errors Gracefully
```json
// Common error responses
{"detail": "Clip not found: uuid"}
{"detail": "Layer not found: uuid"}
{"detail": "Move would cause overlap"}
{"detail": "Asset not found: uuid"}
```

---

## Example Workflows

### Workflow 1: Add Avatar Clip

```
1. GET /project/{id}/structure
   → Find avatar layer ID: "layer-avatar-uuid"
   → Find time coverage: ends at 60000ms

2. GET /project/{id}/assets
   → Find avatar asset ID: "asset-avatar-uuid"

3. POST /project/{id}/clips
   {
     "layer_id": "layer-avatar-uuid",
     "asset_id": "asset-avatar-uuid",
     "start_ms": 60000,
     "duration_ms": 30000,
     "scale": 0.5,
     "x": 400,
     "y": -200
   }

4. PATCH /project/{id}/clip/{new-clip-id}/effects
   {
     "chroma_key_enabled": true,
     "chroma_key_color": "#00FF00"
   }
```

### Workflow 2: Close All Gaps

```
1. GET /project/{id}/structure
   → Get all layer IDs

2. For each layer with clip_count > 1:
   POST /project/{id}/semantic
   {
     "operation": "close_gap",
     "target_layer_id": "layer-uuid"
   }
```

### Workflow 3: Sync Audio to Video

```
1. GET /project/{id}/clip/{video-clip-id}
   → Get timing: start_ms=5000, duration_ms=10000

2. GET /project/{id}/structure
   → Find narration track ID

3. POST /project/{id}/audio-clips
   {
     "track_id": "narration-track-uuid",
     "asset_id": "narration-asset-uuid",
     "start_ms": 5000,
     "duration_ms": 10000,
     "group_id": "sync-group-1"
   }

4. Update video clip with same group:
   (Use batch or separate PATCH)
```

### Workflow 4: Rename a Layer

```
1. GET /project/{id}/structure
   → Find layer by name: "レイヤー１" has ID "layer-uuid-1"

2. PATCH /project/{id}/layer/layer-uuid-1
   {
     "name": "ナレーション"
   }
   → Returns updated LayerSummary

Alternative (semantic operation):
POST /project/{id}/semantic
{
  "operation": "rename_layer",
  "target_layer_id": "layer-uuid-1",
  "parameters": {
    "name": "ナレーション"
  }
}
```

---

## Output Specifications

When rendering the final video, Douga produces:

| Property | Value |
|----------|-------|
| Resolution | 1920x1080 (Full HD) |
| Frame Rate | 30fps |
| Video Codec | H.264 |
| Audio Codec | AAC |
| Audio Bitrate | 320kbps |
| Container | MP4 |
| Compatibility | Udemy recommended format |

---

## MCP Server Integration

For AI assistants using the Model Context Protocol (MCP), a FastMCP server is provided.

### Running the MCP Server

```bash
cd backend

# Set environment variables
export DOUGA_API_URL=http://localhost:8000
export DOUGA_API_TOKEN=dev-token

# Run MCP server
python -m src.mcp.server
```

### Available MCP Tools

The MCP server exposes the following tools:

**Read Tools (L1→L2→L3):**
- `get_project_overview(project_id)` - L1 overview
- `get_timeline_structure(project_id)` - L2 structure
- `get_timeline_at_time(project_id, time_ms)` - L2 at time
- `get_asset_catalog(project_id)` - L2 assets
- `get_clip_details(project_id, clip_id)` - L3 video clip
- `get_audio_clip_details(project_id, clip_id)` - L3 audio clip

**Layer Management Tools:**
- `add_layer(project_id, name, layer_type, insert_at)` - Create new layer
- `update_layer(project_id, layer_id, name, visible, locked)` - Update layer properties (rename, visibility, lock)
- `reorder_layers(project_id, layer_ids)` - Reorder layers

**Write Tools:**
- `add_clip(...)` - Add video clip
- `move_clip(...)` - Move video clip
- `update_clip_transform(...)` - Update transform
- `update_clip_effects(...)` - Update effects
- `delete_clip(...)` - Delete video clip
- `add_audio_clip(...)` - Add audio clip
- `move_audio_clip(...)` - Move audio clip
- `delete_audio_clip(...)` - Delete audio clip

**Semantic Operations:**
- `snap_to_previous(project_id, target_clip_id)` - Close gap to previous
- `snap_to_next(project_id, target_clip_id)` - Close gap from next
- `close_gap(project_id, target_layer_id)` - Remove all gaps in layer
- `auto_duck_bgm(project_id, ...)` - Enable BGM ducking
- `rename_layer(project_id, layer_id, new_name)` - Rename a layer

**Analysis:**
- `analyze_gaps(project_id)` - Find gaps
- `analyze_pacing(project_id, segment_duration_ms)` - Analyze pacing

**Preview & Inspection:**
- `get_event_points(project_id, ...)` - Detect key timeline moments
- `sample_frame(project_id, time_ms, resolution)` - Render single preview frame
- `sample_event_points(project_id, max_samples, ...)` - Auto-detect + render frames
- `validate_composition(project_id, rules)` - Rule-based composition check

---

## Preview & Inspection

AI-driven visual quality control without full renders.

### Event Point Detection

Detect key moments in the timeline for targeted visual inspection.

```
POST /api/projects/{id}/preview/event-points
```

**Request:**
```json
{
  "include_audio": true,
  "include_visual": true,
  "min_gap_ms": 500
}
```

**Response:**
```json
{
  "project_id": "...",
  "event_points": [
    {
      "time_ms": 5000,
      "event_type": "slide_change",
      "description": "Content: slide clip starts",
      "layer": "content",
      "clip_id": "clip-uuid",
      "metadata": {"start_ms": 5000, "duration_ms": 10000}
    }
  ],
  "total_events": 15,
  "duration_ms": 60000
}
```

**Event Types (13):**
- `clip_start`, `clip_end` - Generic clip boundaries
- `slide_change` - Content/slide transitions
- `section_boundary` - Multiple layers change simultaneously
- `avatar_enter`, `avatar_exit` - Avatar appearance
- `narration_start`, `narration_end` - Narration audio
- `bgm_start` - Background music
- `se_trigger` - Sound effect insertion
- `silence_gap` - No audio across all tracks
- `effect_point` - Effects layer activity
- `layer_change` - Layer composition change

### Frame Sampling

Render a single preview frame at a specific time.

```
POST /api/projects/{id}/preview/sample-frame
```

**Request:**
```json
{
  "time_ms": 5000,
  "resolution": "640x360"
}
```

**Response:**
```json
{
  "time_ms": 5000,
  "resolution": "640x360",
  "frame_base64": "/9j/4AAQ...",
  "size_bytes": 45230
}
```

### Combined Event Point Sampling

Auto-detect events and render preview frames in one call.

```
POST /api/projects/{id}/preview/sample-event-points
```

**Request:**
```json
{
  "max_samples": 10,
  "resolution": "640x360",
  "include_audio": true,
  "min_gap_ms": 500
}
```

**Response:**
```json
{
  "project_id": "...",
  "samples": [
    {
      "time_ms": 5000,
      "event_type": "section_boundary",
      "description": "Section boundary: 3 layer changes",
      "frame_base64": "/9j/4AAQ..."
    }
  ],
  "total_events": 25,
  "sampled_count": 10
}
```

### Composition Validation

Check timeline composition rules without rendering.

```
POST /api/projects/{id}/preview/validate
```

**Request:**
```json
{
  "rules": null
}
```

Pass `null` to run all rules, or specify a subset:

```json
{
  "rules": ["safe_zone", "text_readability", "gap_detection"]
}
```

**Available Rules (10):**
- `overlapping_clips` - Same-layer clip overlaps
- `clip_bounds` - Clips extending beyond timeline
- `missing_assets` - Referenced assets not found
- `safe_zone` - Text/avatar outside 5% margin
- `empty_layers` - Visible layers with no clips
- `audio_sync` - Narration without visual content
- `duration_consistency` - Timeline vs content duration mismatch
- `text_readability` - Text display time / font size issues
- `layer_ordering` - Non-standard layer order
- `gap_detection` - Blank screen periods

**Response:**
```json
{
  "project_id": "...",
  "is_valid": false,
  "issues": [
    {
      "rule": "safe_zone",
      "severity": "warning",
      "message": "Text extends outside safe zone (right)",
      "time_ms": 5000,
      "clip_id": "clip-uuid",
      "layer": "text",
      "suggestion": "Adjust position or scale to stay within 5% margin"
    }
  ],
  "total_issues": 3,
  "errors": 0,
  "warnings": 3
}
```

### Recommended AI Workflow

```
apply_plan()              → Generate timeline from plan
validate_composition()    → Pre-flight rule checks (fast, no render)
sample_event_points(10)   → Visual inspection at key moments
[fix issues]              → edit_timeline() for corrections
sample_frame(time_ms)     → Re-check only affected points
render_video()            → Final render when satisfied
```

---

## Parameter Reference (Complete)

All parameters with their exact constraints. AI assistants should respect these ranges.

### Transform Parameters

| Parameter | Type | Min | Max | Step | Default | Unit | Notes |
|-----------|------|-----|-----|------|---------|------|-------|
| x | float | -3840 | 3840 | 1 | null | px | From canvas center (1920x1080). |
| y | float | -2160 | 2160 | 1 | null | px | From canvas center (1920x1080). |
| width | float | 1 | 7680 | 1 | null | px | update_transform only. null = use asset width |
| height | float | 1 | 4320 | 1 | null | px | update_transform only. null = use asset height |
| scale | float | 0.01 | 10.0 | 0.01 | null | multiplier | Uniform scale. |
| rotation | float | -360 | 360 | 0.1 | null | degrees | Clockwise rotation |
| anchor | enum | - | - | - | "center" | - | center, top-left, top-right, bottom-left, bottom-right |

### Effects Parameters

| Parameter | Type | Min | Max | Step | Default | Unit | Notes |
|-----------|------|-----|-----|------|---------|------|-------|
| opacity | float | 0.0 | 1.0 | 0.01 | 1.0 | ratio | 0=transparent, 1=opaque |
| blend_mode | enum | - | - | - | "normal" | - | normal, multiply, screen, overlay |
| chroma_key_enabled | bool | - | - | - | false | - | Enable green screen removal |
| chroma_key_color | string | - | - | - | "#00FF00" | hex | Key color to remove |
| chroma_key_similarity | float | 0.0 | 1.0 | 0.01 | 0.4 | ratio | Color match threshold |
| chroma_key_blend | float | 0.0 | 1.0 | 0.01 | 0.1 | ratio | Edge blending amount |

### Transition Parameters (Target)

| Parameter | Type | Min | Max | Step | Default | Unit | Notes |
|-----------|------|-----|-----|------|---------|------|-------|
| transition_in/out.type | enum | - | - | - | cut | - | cut, fade, crossfade, dip_to_black, dip_to_white, wipe_left/right/up/down |
| transition_in/out.duration_ms | int | 0 | 2000 | 1 | 0 | ms | cut=0, non-cut=100-2000 |

### Timing Parameters

| Parameter | Type | Min | Max | Step | Default | Unit | Notes |
|-----------|------|-----|-----|------|---------|------|-------|
| start_ms | int | 0 | - | 1 | required | ms | No explicit max; project duration auto-extends |
| duration_ms | int | 1 | 3600000 | 1 | required | ms | Max 1 hour |
| in_point_ms | int | 0 | asset_duration | 1 | 0 | ms | Start trim in source |
| out_point_ms | int | 0 | asset_duration | 1 | null | ms | End trim. null = asset end |

### Audio Parameters

| Parameter | Type | Min | Max | Step | Default | Unit | Notes |
|-----------|------|-----|-----|------|---------|------|-------|
| volume | float | 0.0 | 2.0 | 0.01 | 1.0 | multiplier | 0=mute, 1=normal, 2=boost |
| fade_in_ms | int | 0 | 10000 | 1 | 0 | ms | Fade in duration |
| fade_out_ms | int | 0 | 10000 | 1 | 0 | ms | Fade out duration |

### Practical Notes (No API warnings)

The current API does **not** emit warning objects. If you need guardrails,
apply these checks client-side:

- scale very small/large may reduce visibility or quality
- large |x|/|y| may move clip off-screen
- volume > 1.0 may cause clipping

---

## Error Handling (Current)

The current API returns a simple `detail` string with an HTTP status code.
There are **no** structured error codes, suggested actions, or warnings yet.

### Examples

```json
{"detail": "Project not found"}
{"detail": "Layer not found: 1f2a..."}
{"detail": "Clip not found: 6b0e..."}
{"detail": "Audio clip not found: 7c9d..."}
{"detail": "Asset not found: 9c3d..."}
{"detail": "out_point_ms (12000) exceeds asset duration (10000)"}
{"detail": "in_point_ms (8000) must be less than out_point_ms (7000)"}
```

### Recovery Guidelines

- If an ID is not found, refresh with `GET /structure` or `GET /assets`.
- If timing is invalid, adjust `in_point_ms` / `out_point_ms` / `duration_ms`.
- If an operation fails mid-batch, inspect `results` and `errors` arrays.

---

## Validation Mode (Future)

> **Note:** This section describes planned functionality for validate_only mode.
> Check API version for availability.

### Purpose

Pre-validate operations without executing them. Catch errors before they happen.

### Usage

Add `?validate_only=true` to any write endpoint:

```
POST /api/ai/project/{id}/clips?validate_only=true
```

### Response (Validation Success)

```json
{
  "valid": true,
  "can_execute": true,
  "computed_values": {
    "start_ms": 15000,
    "end_ms": 20000,
    "snapped_to": 15000
  },
  "would_affect": {
    "project_duration_change_ms": 0,
    "clips_created": 1,
    "clips_moved": 0,
    "layers_affected": ["layer-uuid"]
  },
  "warnings": [],
  "conflicts": []
}
```

### Response (Validation with Warnings)

```json
{
  "valid": true,
  "warnings": [
    {
      "code": "WARN_SMALL_SCALE",
      "field": "transform.scale",
      "value": 0.05,
      "message": "Scale 0.05 will make clip very small (5%)",
      "recommendation": "Use scale >= 0.1 for better visibility"
    }
  ]
}
```

### Response (Validation Failure)

```json
{
  "valid": false,
  "conflicts": [
    {
      "code": "CNF_OVERLAP",
      "message": "Would overlap with clip-existing at 14000-17000ms",
      "overlap_ms": 2000,
      "suggested_fix": "Set start_ms >= 17000"
    }
  ]
}
```

---

## Rollback & History (Future)

> **Note:** This section describes planned functionality for operation history and rollback.
> Check API version for availability.

### Operation History

Every write operation is logged with before/after state:

```
GET /api/ai/project/{id}/history?limit=20
```

Response:
```json
{
  "operations": [
    {
      "operation_id": "op-20240115-150000-abc",
      "timestamp": "2024-01-15T15:00:00Z",
      "actor": {"type": "ai", "name": "Claude"},
      "type": "add_clip",
      "description": "Added clip to Avatar layer",
      "rollback_token": "rb-20240115-150000-abc",
      "rollback_available": true
    }
  ]
}
```

### Rollback Single Operation

```
POST /api/ai/project/{id}/rollback
{
  "rollback_token": "rb-20240115-150000-abc"
}
```

Response:
```json
{
  "success": true,
  "rolled_back": {
    "operation_id": "op-20240115-150000-abc",
    "type": "add_clip"
  },
  "reverted_changes": {
    "clips_removed": ["clip-new-xyz"]
  },
  "redo_token": "redo-20240115-150500-xyz"
}
```

### Snapshots

Create named restore points:

```
POST /api/ai/project/{id}/snapshots
{
  "name": "Before AI editing",
  "description": "Manual checkpoint"
}
```

Restore to snapshot:

```
POST /api/ai/project/{id}/snapshots/{snapshot_id}/restore
```

---

## AI Best Practices (Extended)

### 6. Validate Composition (Current)

```
1. After edits, POST /api/projects/{id}/preview/validate-composition
2. Review issues (errors/warnings/info)
3. Fix issues with targeted edits
```

### 7. No API Rollback (Current)

There is no rollback token in the current API.
If you need undo, store a local snapshot of timeline_data or instruct the user
to use the UI undo/redo.

### 8. Report Changes Clearly

After any write operation, tell the user:
- What changed (diff summary)
- Any validation issues from validate_composition (if checked)

### 9. Never Guess IDs

```
❌ Bad:  Use clip ID from memory or previous session
✅ Good: GET /structure → find current ID → use it
```

### 10. Prefer Semantic Operations

```
❌ Bad:  Calculate start_ms manually, then PATCH /move
✅ Good: POST /semantic with snap_to_previous
```

---

## Ambiguity Resolution & Confidence

> **Critical for AI reliability.** AI must know when to ask vs execute.

### Confidence Levels

| Level | Score | Indicator | Action |
|-------|-------|-----------|--------|
| High | 85-100% | Clear parameters, single interpretation | Execute |
| Medium | 70-84% | Missing 1-2 details | Ask focused question |
| Low | 50-69% | Multiple valid interpretations | Present options |
| Very Low | <50% | Vague or ambiguous | Clarify before any action |

### Common Ambiguous Instructions

| User Says | Problem | Ask |
|-----------|---------|-----|
| "いい感じにして" | No criteria | "何を改善しますか？タイミング/ビジュアル/全体構成?" |
| "大きくして" | No value | "どのくらい？scale=2.0? 画面幅いっぱい?" |
| "タイミング調整" | No reference | "何と同期？前のクリップ？ナレーション？" |
| "ここに追加" | Ambiguous "here" | "再生ヘッド位置（{time}秒）に追加？" |
| "削除して" | Target unclear | "どのクリップを削除？[A, B, C]" |

### Multi-Interpretation Pattern

When multiple valid interpretations exist:

```
User: "クリップを中央に配置"

Interpretation A: x=0 only (horizontal center)
Interpretation B: x=0, y=0 (complete center)
Interpretation C: Distribute evenly in layer

Default: A (least disruptive)
Ask if: B or C would be significantly different outcome
```

### Confirmation Template

```
[Operation] を実行します：
- 対象: [clip/layer name]
- 変更: [parameter] を [old_value] → [new_value]
- 影響: [duration change / position change / etc.]

実行してよろしいですか？
```

### Confirmation Thresholds (Must Ask)

| Change | Threshold | Ask? |
|--------|-----------|------|
| scale | <= 0.5 または >= 2.0 | Yes |
| rotation | >= 30° | Yes |
| position | 画面幅/高さの 25%以上移動（x>=480px or y>=270px） | Yes |
| batch | 5件以上 or 3レイヤー以上に影響 | Yes |
| delete | すべての削除操作 | Yes |

---

## Multi-turn Conversation Context

> AI must maintain context across turns to understand "it", "that", "there".

### Session State (Conceptual)

```json
{
  "session_id": "session-abc123",
  "turn_count": 5,
  "last_operation": {
    "operation_id": "op-xyz",
    "type": "add_clip",
    "affected_clip_id": "clip-new-123",
    "affected_layer_id": "layer-avatar",
    "timestamp": "2024-01-15T15:00:00Z"
  },
  "pronoun_references": {
    "それ/それ/this/that": "clip-new-123",
    "ここ/そこ/here/there": "layer-avatar OR playhead_ms"
  },
  "active_selection": ["clip-new-123"]
}
```

### Pronoun Resolution Rules

| Pronoun | Resolution Priority |
|---------|-------------------|
| "それ" / "that" | 1. Last affected clip 2. Last mentioned clip 3. Ask |
| "ここ" / "here" | 1. Playhead position 2. Last affected layer 3. Ask |
| "もっと" / "more" | Increase last changed parameter by 1.5x |
| "戻して" / "undo" | Rollback last operation |

### Example Multi-turn

```
T1: User "アバター追加して"
    AI: [Add clip] → last_clip = "clip-abc"

T2: User "もっと大きく"
    AI: [Resolve "それ" → clip-abc] [scale 1.0 → 1.5]

T3: User "左に"
    AI: [Resolve "それ" → clip-abc] [x 0 → -200]

T4: User "最初のやつに戻して"
    AI: Ask which change to revert, then apply a compensating edit
```

---

## Edge Cases

> Explicit handling of boundary conditions.

### Timing Edge Cases

| Condition | Status | Response |
|-----------|--------|----------|
| duration_ms = 0 | **Error** | 422 validation error |
| duration_ms < 0 | **Error** | 422 validation error |
| start_ms < 0 | **Error** | 422 validation error |
| in_point_ms >= out_point_ms | **Error** | 400 detail string |
| clip_end > project_duration | **OK** | project duration auto-extends |

### Transform Edge Cases

| Condition | Status | Response |
|-----------|--------|----------|
| scale = 0 | **Error** | 422 validation error |
| scale < 0.01 | **Error** | 422 validation error |
| scale > 10.0 | **Error** | 422 validation error |
| abs(x) > 960 | **OK** | no warning (off-screen possible) |
| abs(x) > 1920 | **OK** | no warning (off-screen possible) |

### Layer Edge Cases

| Condition | Status | Response |
|-----------|--------|----------|
| Add clip to empty layer | **OK** | Normal 201 |
| Add clip to locked layer | **OK** | lock not enforced in AI API |
| Delete last clip in layer | **OK** | Normal 204 (layer remains) |
| Move clip to non-existent layer | **Error** | 400 detail string |

### Audio Edge Cases

| Condition | Status | Response |
|-----------|--------|----------|
| volume = 0 | **OK** | (muted) |
| volume > 2.0 | **Error** | 422 validation error |
| volume > 1.5 | **OK** | no warning; may clip |
| fade_in + fade_out > duration | **OK** | no warning; fades may overlap |

---

## Partial Failure Handling

> Batch operations may partially succeed.

### Behavior (Current)

Batch operations are **best-effort only**. The API returns a summary and per-op
results, but does **not** support atomic mode or rollback.

### Partial Failure Response (Current)

```json
{
  "success": false,
  "total_operations": 3,
  "successful_operations": 2,
  "failed_operations": 1,
  "results": [
    { "operation": "add", "clip_id": "clip-1" },
    { "operation": "move", "clip_id": "clip-2" },
    { "operation": "update_transform", "error": "clip_id required for update_transform" }
  ],
  "errors": [
    "Operation update_transform failed: clip_id required for update_transform"
  ]
}
```

### Recovery Strategy

1. **Review results** - Identify which operations succeeded
2. **Fix failed operations** - Correct inputs and retry only failed ops
3. **Report to user** - Explain partial success clearly

### Dependency-Aware Rules

- **後続が依存**している失敗（例: add_clip が失敗し、その clip_id を使う操作が後続にある）  
  → 後続は **中止**、修正後に再実行
- **独立した失敗**（例: 追加とは別の clip の色変更が失敗）  
  → 失敗分のみ再試行
- **削除の失敗**（対象不明）  
  → `GET /structure` で確認してから再実行

### Partial Failure Reporting Template

```
⚠️ 一部成功しました

成功: {successful}/{total}
失敗: {failed}
影響: {affected_layers/clips}

失敗内容:
- {op} → {error}

次の選択肢:
1) 失敗分のみ再試行
2) 状態確認（GET /structure / validate_composition）
3) すべて取り消し（可能なら rollback / UI undo）
```

---

## Failure Modes & Recovery Catalog

> Comprehensive list of failure scenarios and recovery procedures.

### Network & Communication Failures

| Failure | Detection | Recovery |
|---------|-----------|----------|
| Request timeout | HTTP 504 or timeout | Retry with smaller batch or wait and retry |
| Connection lost mid-operation | No response | GET /structure to verify state, retry if needed |
| Rate limited | HTTP 429 | Wait for `retry_after` header, then retry |
| Auth token expired | 401 Unauthorized | Refresh token, retry request |

### Data Integrity Failures

| Failure | Detection | Recovery |
|---------|-----------|----------|
| ID mismatch | 404 or `detail` contains "not found" | GET /structure to refresh IDs |
| Stale data (concurrent edit) | 404 or unexpected state | Refresh state, re-plan operations |
| Corrupt response | JSON parse error | Retry request, report if persists |
| Incomplete batch | Partial results | Check successful_count, handle individually |

### Operation-Specific Failures

| Operation | Common Failure | Recovery |
|-----------|----------------|----------|
| Add clip | "Asset not found" or timing error | Refresh assets / adjust timing |
| Move clip | "Layer/Track not found" | Refresh structure, retry |
| Delete clip | "Clip not found" | Refresh structure (may already be deleted) |
| Transform | 422 validation error | Check parameter limits in spec |
| Batch | Partial failure | Process results array individually |

### Recovery Decision Tree

```
Error received
    │
    ├─ Is it a 4xx validation error?
    │     └─ Yes → Fix input, retry immediately
    │
    ├─ Does `detail` include "not found"?
    │     └─ Yes → GET /structure or /assets, retry with valid IDs
    │
    ├─ Is it a 5xx?
    │     └─ Yes → Wait, retry (with backoff)
    │
    └─ Unknown error
          └─ Report to user, ask for guidance
```

### Auto-Recovery Patterns (Recommended)

| Error Pattern | Auto Action | Fallback |
|--------------|-------------|----------|
| 404 Not Found (clip/layer/asset) | GET /structure or /assets, re-resolve IDs | Ask user to confirm target |
| TIMELINE_OVERLAP | Use snap_to_previous/next or close_gap | Ask user if overlap is intended |
| INVALID_TIME_RANGE | Adjust out_point_ms to > in_point_ms | Ask for desired trim |
| UNSUPPORTED_FONT_FAMILY | Fetch /capabilities, pick nearest | Ask for preferred font |

### Retry Policy

| Error Type | Max Retries | Backoff | Notes |
|------------|-------------|---------|-------|
| 5xx/timeout | 3 | Exponential (1s, 2s, 4s) | Reduce batch size on 2nd retry |
| 429 rate limit | 5 | Use retry_after | Don't reduce batch |
| 401 auth | 1 | None | Refresh token, retry |
| 404 not found | 2 | None | Refresh IDs between retries |
| 400/422 validation | 1 | None | Fix input, single retry |

### Critical Failure Escalation

When to stop retrying and escalate to user:

1. Same error 3+ times with different approaches
2. Unknown error received
3. State is inconsistent (structure doesn't match expected)
4. User explicitly requested feedback before continuing
5. Operation involves deletion or irreversible changes

### Failure Reporting Template

```
❌ 操作が失敗しました

エラー: {detail}
影響: {affected_clips/layers}

選択肢:
1. {fix_option_1}
2. {fix_option_2}
3. キャンセル（変更なし）

どうしますか？
```

---

## Concurrent Editing

> User may edit manually while AI is planning.

### Problem

```
T1: AI reads L2 structure (version v1)
T2: User manually deletes clip-abc
T3: AI tries to move clip-abc → 404 Not Found
```

### Solution: Optimistic Locking (Future)

```http
GET /api/ai/project/{id}/structure
→ Response includes: "version": "v-20240115-150000"

POST /api/ai/project/{id}/clips
If-Match: "v-20240115-150000"
→ 409 Conflict if version changed
```

### Conflict Response

```json
{
  "error": {
    "code": "CNF_VERSION_MISMATCH",
    "message": "Project was modified since version v-20240115-150000",
    "current_version": "v-20240115-150500",
    "changes_since": [
      { "type": "clip_deleted", "clip_id": "clip-abc", "by": "user" }
    ],
    "suggested_fix": "Refresh structure with GET /structure and retry"
  }
}
```

### Best Practice (Current)

Without optimistic locking:
1. Keep AI operations fast (minimize time between read and write)
2. Re-read L2 structure before complex batch operations
3. Handle 404 errors gracefully (re-fetch and retry)

---

## Large Projects

> Projects with 100+ clips require special handling.

### Token Budget Problem

| Data | Tokens (approx) |
|------|-----------------|
| L1 Overview | ~300 |
| L2 Structure | ~800 (for 20 clips) |
| L2 Structure | ~4000 (for 100 clips) |
| L3 per clip | ~400 |

### Mitigation Strategies (Current)

- Ask the user which section to focus on (intro/main/outro) and limit edits there
- Use L2 structure once, then fetch only the needed clips via L3 endpoints
- For visual verification, sample key frames with `sample_event_points`

> Note: The current `/structure` endpoint does **not** support filter or pagination

### Recommendation

For projects > 50 clips:
1. Start with L1 overview to understand scale
2. Ask user which section to focus on
3. Filter L2 to that section only
4. Process incrementally

---

## Frame Precision

> 30fps = 33.33ms per frame. Millisecond precision has limits.

### Frame-Millisecond Relationship

| FPS | Frame Duration | Precision |
|-----|---------------|-----------|
| 30 | 33.33ms | ±16.67ms |
| 60 | 16.67ms | ±8.33ms |
| 24 | 41.67ms | ±20.83ms |

### Rounding Rules (Current)

- **API Input**: Accepts integer milliseconds only
- **Internal**: No automatic frame snapping (values are used as-is)
- **Output**: Returns the same millisecond values stored in timeline

### Practical Impact

```
AI sets: start_ms = 5000
30fps frame: 5000 / 33.33 = 150.0 frames → exact
AI sets: start_ms = 5010
Result: start_ms remains 5010 (no automatic rounding)
```

### Best Practice

1. If you want frame alignment, use multiples of frame duration:
   - 30fps: 0, 33, 67, 100, 133, ... ms
2. Use semantic operations to avoid manual arithmetic mistakes

---

## Visual Verification

> AI cannot see rendered previews directly.

### The Problem

AI operates on JSON data but cannot verify:
- Is the clip actually visible?
- Does the chroma key look correct?
- Is text readable?

### Solution: Preview Sampling

Use `sample_frame` to get visual confirmation:

```
POST /api/projects/{id}/preview/sample-frame
{ "time_ms": 5000, "resolution": "640x360" }

→ Returns base64 JPEG image
```

### When to Sample

| Situation | Sample? |
|-----------|---------|
| Transform warnings (off-screen, tiny scale) | Yes |
| Chroma key enabled | Yes, at clip start |
| Text clip added | Yes |
| Complex batch complete | Yes, at key moments |
| Simple move operation | No (trust the math) |

### Verification Flow

```
1. Execute operation
2. Check for warnings in response
3. If warnings OR user requests verification:
   - sample_frame at affected time
   - Report to user: "Here's how it looks: [image]"
4. Offer adjustment if needed
```

---

## Udemy-Specific Requirements

> Douga is designed for creating Udemy course videos. Follow these platform constraints.

### Output Specifications (Udemy Compatible)

| Property | Value | Notes |
|----------|-------|-------|
| Resolution | 1920×1080 | Full HD, mandatory |
| Aspect ratio | 16:9 | Required |
| Frame rate | 30fps | Constant frame rate |
| Video codec | H.264 | High profile |
| Audio codec | AAC | Stereo, 48kHz |
| Audio bitrate | 192-320 kbps | AAC-LC recommended |
| Container | MP4 | Udemy required format |
| Bitrate | 8-12 Mbps | For good quality |

### Safe Zone Guidelines

```
┌──────────────────────────────────────────────┐
│          ← 5% margin (96px) →                │
│  ┌────────────────────────────────────────┐  │
│  │                                        │  │
│  │         SAFE ZONE FOR TEXT             │  │
│  │         Keep important content         │  │
│  │         within this area               │  │
│  │                                        │  │
│  │  ← Keep avatar here for consistency →  │  │
│  │                                        │  │
│  └────────────────────────────────────────┘  │
│     Bottom 10% reserved for progress bar     │
└──────────────────────────────────────────────┘
```

### Text Readability

| Element | Minimum Size | Recommended |
|---------|--------------|-------------|
| Main title | 48px | 64px |
| Subtitle | 32px | 40px |
| Body text | 24px | 32px |
| Code/terminal | 18px | 24px (monospace) |

**Color Contrast:**
- White text on dark background: Preferred
- Avoid pure white (#FFFFFF) on pure black (#000000) - use #F5F5F5 on #1A1A1A
- Minimum contrast ratio: 4.5:1

### Lecture Structure Patterns

| Section | Duration | Content |
|---------|----------|---------|
| Intro | 5-10s | Title + avatar greeting |
| Agenda | 5-15s | What will be covered |
| Main content | Variable | Screencasts, slides, demos |
| Transitions | 1-3s | Section dividers |
| Outro | 5-10s | Summary + next steps |

### Common Avatar Positions

| Position | Coordinates | Use Case |
|----------|-------------|----------|
| Center | x=0, y=0, scale=1.0 | Intro, full-screen talking |
| Bottom-right | x=600, y=350, scale=0.4 | Screencast overlay |
| Bottom-left | x=-600, y=350, scale=0.4 | Alternative overlay |
| Off-screen left | x=-1200, y=0 | Avatar exit |

### BGM Guidelines

| Scenario | BGM Volume | Ducking |
|----------|------------|---------|
| No narration | 0.3-0.5 | N/A |
| With narration | 0.05-0.1 | auto_duck_bgm |
| Intro/Outro | 0.4-0.6 | No narration typically |
| Intense section | 0.0 | Mute BGM |

### Audio Loudness Targets (Udemy)

| Metric | Target | Notes |
|--------|--------|-------|
| Integrated Loudness | -19 LUFS ±1 | Course audio consistency |
| True Peak | <= -1 dBTP | Avoid clipping |
| Narration | -19 LUFS | Primary voice |
| BGM | -28 to -24 LUFS | Lower than narration |
| Ducking (during narration) | ~ -32 LUFS | Auto-ducked BGM target |

### Udemy-Specific Checks (Current)

Use `POST /api/projects/{id}/preview/validate-composition` and review the
`issues` list. Common rules include:

| Rule | Description |
|------|-------------|
| overlapping_clips | Clips overlap on the same layer |
| clip_bounds | Clip is outside canvas bounds |
| missing_assets | Clip references missing asset |
| safe_zone | Text outside safe zone |
| text_readability | Text too small / low contrast |
| gap_detection | Unexpected gaps in timeline |

### Pre-Export Checklist

Before exporting for Udemy:

```
1. [ ] Resolution: 1920×1080
2. [ ] Audio: No clipping, consistent levels
3. [ ] Text: All text readable at 720p
4. [ ] Avatar: Visible when speaking
5. [ ] Transitions: Smooth, not jarring
6. [ ] Duration: Under 15 minutes (or split)
7. [ ] Safe zones: Content not cut off
```

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.6.0 | 2026-02 | Added confirmation thresholds, partial-failure templates, audio loudness targets, transition parameters, auto-recovery patterns |
| 1.5.0 | 2026-02 | Added Schema Map, Terminology Definitions, Udemy Requirements, Failure Modes Catalog, Suggested Actions structure |
| 1.4.0 | 2026-02 | Added Ambiguity Resolution, Multi-turn Context, Edge Cases, Partial Failure, Concurrent Editing, Large Projects, Frame Precision, Visual Verification |
| 1.3.0 | 2026-02 | Added Parameter Reference, Error Codes, Validation Mode (planned), Rollback (planned) |
| 1.2.0 | 2026-01 | Added Preview & Inspection API (event points, frame sampling, composition validation, interpolation) |
| 1.1.0 | 2025-01 | Added MCP server, improved validation |
| 1.0.0 | 2025-01 | Initial AI integration API |

---

End of specification.
